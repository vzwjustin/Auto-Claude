# Auto Claude Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# AUTHENTICATION (REQUIRED)
# =============================================================================
# Auto Claude uses Claude Code OAuth authentication.
# Direct API keys (ANTHROPIC_API_KEY) are NOT supported to prevent silent billing.
#
# Option 1: Run `claude setup-token` to save token to macOS Keychain (recommended)
# Option 2: Set the token explicitly:
# CLAUDE_CODE_OAUTH_TOKEN=your-oauth-token-here
#
# For enterprise/proxy setups (CCR):
# ANTHROPIC_AUTH_TOKEN=sk-zcf-x-ccr

# =============================================================================
# CUSTOM API ENDPOINT (OPTIONAL)
# =============================================================================
# Override the default Anthropic API endpoint. Useful for:
#   - Local proxies (ccr, litellm)
#   - API gateways
#   - Self-hosted Claude instances
#
# ANTHROPIC_BASE_URL=http://127.0.0.1:3456
#
# Related settings (usually set together with ANTHROPIC_BASE_URL):
# NO_PROXY=127.0.0.1
# DISABLE_TELEMETRY=true
# DISABLE_COST_WARNINGS=true
# API_TIMEOUT_MS=600000

# Model override (OPTIONAL)
# Default: claude-opus-4-5-20251101
# AUTO_BUILD_MODEL=claude-opus-4-5-20251101


# =============================================================================
# GIT/WORKTREE SETTINGS (OPTIONAL)
# =============================================================================
# Configure how Auto Claude handles git worktrees for isolated builds.

# Default base branch for worktree creation (OPTIONAL)
# If not set, Auto Claude will auto-detect main/master, or fall back to current branch.
# Common values: main, master, develop
# DEFAULT_BRANCH=main

# =============================================================================
# DEBUG MODE (OPTIONAL)
# =============================================================================
# Enable debug logging for development and troubleshooting.
# Shows detailed information about runner execution, agent calls, file operations.

# Enable debug mode (default: false)
# DEBUG=true

# Debug log level: 1=basic, 2=detailed, 3=verbose (default: 1)
# DEBUG_LEVEL=1

# Log to file instead of stdout (OPTIONAL)
# DEBUG_LOG_FILE=auto-claude/debug.log

# =============================================================================
# LINEAR INTEGRATION (OPTIONAL)
# =============================================================================
# Enable Linear integration for real-time progress tracking in Linear.
# Get your API key from: https://linear.app/YOUR-TEAM/settings/api

# Linear API Key (OPTIONAL - enables Linear integration)
# LINEAR_API_KEY=lin_api_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Pre-configured Team ID (OPTIONAL - will auto-detect if not set)
# LINEAR_TEAM_ID=

# Pre-configured Project ID (OPTIONAL - will create project if not set)
# LINEAR_PROJECT_ID=

# =============================================================================
# UI SETTINGS (OPTIONAL)
# =============================================================================
# Enable fancy terminal UI with icons, colors, and interactive menus.
# Set to "false" to use plain text output (useful for CI/CD or log files).

# Enable fancy UI (default: true)
# ENABLE_FANCY_UI=true

# =============================================================================
# ELECTRON MCP SERVER (OPTIONAL)
# =============================================================================
# Enable Electron MCP server for AI agents to interact with and validate
# Electron desktop applications. This allows QA agents to capture screenshots,
# inspect windows, and validate Electron apps during the review process.
#
# The electron-mcp-server connects via Chrome DevTools Protocol to an Electron
# app running with remote debugging enabled.
#
# Prerequisites:
#   1. Start your Electron app with remote debugging:
#      ./YourElectronApp --remote-debugging-port=9222
#
#   2. For auto-claude-ui specifically (use the MCP-enabled scripts):
#      cd auto-claude-ui
#      pnpm run dev:mcp     # Development mode with MCP debugging
#      # OR for production build:
#      pnpm run start:mcp   # Production mode with MCP debugging
#
# Note: Only QA agents (qa_reviewer, qa_fixer) receive Electron MCP tools.
# Coder and Planner agents do NOT have access to these tools to minimize
# context token usage and keep agents focused on their roles.
#
# See: https://github.com/anthropics/anthropic-quickstarts/tree/main/mcp-electron-demo

# Enable Electron MCP integration (default: false)
# ELECTRON_MCP_ENABLED=true

# Chrome DevTools debugging port for Electron connection (default: 9222)
# ELECTRON_DEBUG_PORT=9222

# =============================================================================
# GRAPHITI MEMORY INTEGRATION V2 (OPTIONAL)
# =============================================================================
# Enable Graphiti-based persistent memory layer for cross-session context
# retention. Uses FalkorDB as the graph database backend.
#
# V2 supports multiple LLM and embedder providers:
#   - OpenAI (default)
#   - Anthropic (LLM only, use with Voyage for embeddings)
#   - Azure OpenAI
#   - Ollama (local, fully offline)
#
# Prerequisites:
#   1. Start FalkorDB: docker-compose up -d falkordb
#   2. Install Graphiti: pip install graphiti-core[falkordb]
#   3. Configure your chosen provider (see below)

# Enable Graphiti integration (default: false)
# GRAPHITI_ENABLED=true

# =============================================================================
# GRAPHITI: Provider Selection
# =============================================================================
# Choose which providers to use for LLM and embeddings.
# Default is "openai" for both.

# LLM provider: openai | anthropic | azure_openai | ollama | google
# GRAPHITI_LLM_PROVIDER=openai

# Embedder provider: openai | voyage | azure_openai | ollama | google
# GRAPHITI_EMBEDDER_PROVIDER=openai

# =============================================================================
# GRAPHITI: OpenAI Provider (Default)
# =============================================================================
# Use OpenAI for both LLM and embeddings. This is the simplest setup.
# Required: OPENAI_API_KEY

# OpenAI API Key
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# OpenAI Model for LLM (default: gpt-5-mini)
# OPENAI_MODEL=gpt-5-mini

# OpenAI Model for embeddings (default: text-embedding-3-small)
# Available: text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# GRAPHITI: Anthropic Provider (LLM only)
# =============================================================================
# Use Anthropic for LLM. Requires separate embedder (use Voyage or OpenAI).
# Example: GRAPHITI_LLM_PROVIDER=anthropic, GRAPHITI_EMBEDDER_PROVIDER=voyage
#
# Required: ANTHROPIC_API_KEY

# Anthropic API Key
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Anthropic Model (default: claude-sonnet-4-5-latest)
# GRAPHITI_ANTHROPIC_MODEL=claude-sonnet-4-5-latest

# =============================================================================
# GRAPHITI: Voyage AI Provider (Embeddings only)
# =============================================================================
# Use Voyage AI for embeddings. Commonly paired with Anthropic LLM.
# Get API key from: https://www.voyageai.com/
#
# Required: VOYAGE_API_KEY

# Voyage AI API Key
# VOYAGE_API_KEY=pa-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Voyage Embedding Model (default: voyage-3)
# Available: voyage-3 (1024 dim), voyage-3-lite (512 dim)
# VOYAGE_EMBEDDING_MODEL=voyage-3

# =============================================================================
# GRAPHITI: Google AI Provider
# =============================================================================
# Use Google AI (Gemini) for both LLM and embeddings.
# Get API key from: https://aistudio.google.com/apikey
#
# Required: GOOGLE_API_KEY

# Google AI API Key
# GOOGLE_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google LLM Model (default: gemini-2.0-flash)
# GOOGLE_LLM_MODEL=gemini-2.0-flash

# Google Embedding Model (default: text-embedding-004)
# GOOGLE_EMBEDDING_MODEL=text-embedding-004

# =============================================================================
# GRAPHITI: Azure OpenAI Provider
# =============================================================================
# Use Azure OpenAI for both LLM and embeddings.
# Requires Azure OpenAI deployment with appropriate models.
#
# Required: AZURE_OPENAI_API_KEY, AZURE_OPENAI_BASE_URL

# Azure OpenAI API Key
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Azure OpenAI Base URL (your Azure endpoint)
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment

# Azure OpenAI Deployment Names
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-5
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small

# =============================================================================
# GRAPHITI: Ollama Provider (Local/Offline)
# =============================================================================
# Use Ollama for fully offline operation. No API keys required.
# Requires Ollama running locally with appropriate models pulled.
#
# Prerequisites:
#   1. Install Ollama: https://ollama.ai/
#   2. Pull models: ollama pull deepseek-r1:7b && ollama pull nomic-embed-text
#   3. Start Ollama server (usually auto-starts)
#
# Required: OLLAMA_LLM_MODEL, OLLAMA_EMBEDDING_MODEL, OLLAMA_EMBEDDING_DIM

# Ollama Server URL (default: http://localhost:11434)
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama LLM Model
# Popular choices: deepseek-r1:7b, llama3.2:3b, mistral:7b, phi3:medium
# OLLAMA_LLM_MODEL=deepseek-r1:7b

# Ollama Embedding Model
# Popular choices: nomic-embed-text (768 dim), mxbai-embed-large (1024 dim)
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Ollama Embedding Dimension (REQUIRED for Ollama embeddings)
# Must match your embedding model's output dimension
# Common values: nomic-embed-text=768, mxbai-embed-large=1024, all-minilm=384
# OLLAMA_EMBEDDING_DIM=768

# =============================================================================
# GRAPHITI MCP SERVER (OPTIONAL - Agent-accessible knowledge graph)
# =============================================================================
# Enable Graphiti MCP server for direct agent access to knowledge graph.
# This is SEPARATE from GRAPHITI_ENABLED (Python library integration).
#
# With MCP server enabled, Claude agents can directly:
#   - Search for relevant context (search_nodes, search_facts)
#   - Add discoveries to the graph (add_episode)
#   - Retrieve recent sessions (get_episodes)
#
# Quick Start (uses docker-compose.yml in project root):
#   1. Set OPENAI_API_KEY in your .env file (or export it)
#   2. Run: docker-compose up -d
#   3. Set GRAPHITI_MCP_URL below
#
# Manual Docker (alternative):
#   docker run -d -p 8000:8000 \
#     -e DATABASE_TYPE=falkordb \
#     -e OPENAI_API_KEY=$OPENAI_API_KEY \
#     -e FALKORDB_URI=redis://host.docker.internal:6379 \
#     falkordb/graphiti-knowledge-graph-mcp
#
# See: https://docs.falkordb.com/agentic-memory/graphiti-mcp-server.html

# Graphiti MCP Server URL (setting this enables MCP integration)
# GRAPHITI_MCP_URL=http://localhost:8000/mcp/

# Graphiti MCP Server Port (for docker-compose, default: 8000)
# GRAPHITI_MCP_PORT=8000

# Optional: Override model settings for MCP server
# GRAPHITI_MODEL_NAME=gpt-4o-mini
# GRAPHITI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# GRAPHITI: FalkorDB Connection Settings
# =============================================================================
# Configure the FalkorDB graph database connection.
# Start FalkorDB: docker run -p 6379:6379 falkordb/falkordb:latest

# FalkorDB Host (default: localhost)
# GRAPHITI_FALKORDB_HOST=localhost

# FalkorDB Port (default: 6379)
# GRAPHITI_FALKORDB_PORT=6379

# FalkorDB Password (default: empty)
# GRAPHITI_FALKORDB_PASSWORD=

# Graph Database Name (default: auto_claude_memory)
# GRAPHITI_DATABASE=auto_claude_memory

# =============================================================================
# GRAPHITI: Miscellaneous Settings
# =============================================================================

# Disable Graphiti telemetry (default: true)
# GRAPHITI_TELEMETRY_ENABLED=false

# =============================================================================
# GRAPHITI: Example Configurations
# =============================================================================
#
# --- Example 1: OpenAI (simplest) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=openai
# GRAPHITI_EMBEDDER_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxx
#
# --- Example 2: Anthropic + Voyage (high quality) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=anthropic
# GRAPHITI_EMBEDDER_PROVIDER=voyage
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxx
# VOYAGE_API_KEY=pa-xxxxxxxx
#
# --- Example 3: Ollama (fully offline) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=ollama
# GRAPHITI_EMBEDDER_PROVIDER=ollama
# OLLAMA_LLM_MODEL=deepseek-r1:7b
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# OLLAMA_EMBEDDING_DIM=768
#
# --- Example 4: Azure OpenAI (enterprise) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=azure_openai
# GRAPHITI_EMBEDDER_PROVIDER=azure_openai
# AZURE_OPENAI_API_KEY=xxxxxxxx
# AZURE_OPENAI_BASE_URL=https://your-resource.openai.azure.com/...
# AZURE_OPENAI_LLM_DEPLOYMENT=gpt-5
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
#
# --- Example 5: Google AI (Gemini) ---
# GRAPHITI_ENABLED=true
# GRAPHITI_LLM_PROVIDER=google
# GRAPHITI_EMBEDDER_PROVIDER=google
# GOOGLE_API_KEY=AIzaSyxxxxxxxx
